{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMxyNsBrpwPxP5UtlQxFlIP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babaroscopy/Thesis/blob/main/multifiltercnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaAZNOkGgIk7"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.mkdir('/content/normal+bags')\n",
        "os.mkdir('/content/Normal+Coats')\n",
        "os.mkdir('/content/NormalToCoats')\n",
        "os.mkdir('/content/NormalToNormal')\n",
        "os.mkdir('/content/SpeedExperiments')\n",
        "os.mkdir('/content/SpeedExperiments/Normal+Fast')\n",
        "os.mkdir('/content/SpeedExperiments/Normal+Slow')\n",
        "os.mkdir('/content/SpeedExperiments/NormalToNormal')"
      ],
      "metadata": {
        "id": "T0rBr5AUgLO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "from imageio import imread\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from keras import backend as K\n",
        "from keras.preprocessing import image\n",
        "from keras.layers import Dense, Conv2D, MaxPool2D , Flatten,LeakyReLU,BatchNormalization\n",
        "from keras.applications import densenet\n",
        "from keras.regularizers import l1\n",
        "from keras.regularizers import l2\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import ZeroPadding2D\n",
        "from keras import optimizers\n",
        "from matplotlib import pyplot\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy"
      ],
      "metadata": {
        "id": "24sUL7XVgLyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numpy.set_printoptions(threshold=sys.maxsize)\n",
        "X_train=[]\n",
        "Y_train=[]\n",
        "X_test=[]\n",
        "Y_test=[]\n",
        "\n",
        "images1 = \"/content/drive/MyDrive/DatasetGEIs/DatasetGEIs/Normal+Coats/TrainNormal\"\n",
        "files1= os.listdir(images1)\n",
        "#print(files1)\n",
        "X_train = [imread('/content/drive/MyDrive/DatasetGEIs/DatasetGEIs/Normal+Coats/TrainNormal/'+f) for f in files1]   # Getting Training Images\n",
        "for f in files1:\n",
        "    Y_train.append(f[7:10])\n",
        "for i in range(0, len(Y_train)):\n",
        "    Y_train[i] = int(Y_train[i])\n",
        "#print(Y_train)\n",
        "images2 = '/content/drive/MyDrive/DatasetGEIs/DatasetGEIs/Normal+Coats/TestCoats+Normal-C'\n",
        "files2= os.listdir(images2)\n",
        "#print(files2)\n",
        "X_test = [imread(\"/content/drive/MyDrive/DatasetGEIs/DatasetGEIs/Normal+Coats/TestCoats+Normal-C/\"+f) for f in files2]\n",
        "for f in files2:\n",
        "    Y_test.append(f[7:10])\n",
        "\n",
        "for i in range(0, len(Y_test)):\n",
        "    Y_test[i] = int(Y_test[i])\n",
        "#print(Y_test)\n",
        "\n",
        "X_train=np.array(X_train)\n",
        "X_test=np.array(X_test)\n",
        "\n",
        "img_rows=240\n",
        "img_cols=240\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
        "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
        "\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
        "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
        "\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "Y_test=label_encoder.fit_transform(Y_test)\n",
        "Y_train= label_encoder.fit_transform(Y_train)\n",
        "\n",
        "print(Y_train)\n",
        "print(Y_test)\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "Y_train = to_categorical(Y_train)\n",
        "Y_test = to_categorical(Y_test)\n",
        "num_classes = 124\n",
        "epochs = 50\n",
        "MODEL_NAME = 'TrainedModelFile-{}-{}.model'.format(\"0.0001\", 'TenLayers')\n",
        "# Define the custom kernel weights for the first layer\n",
        "first_kernel_weights = [[0,0,0], [0,1,0], [0,0,0]]\n",
        "\n",
        "# Define the custom kernels for different parts of the image\n",
        "kernel1 = np.array([[-1, 0.1, 0.5],\n",
        "                    [-1, 1.5, 0.5],\n",
        "                    [-1, 0.1, 0.5]])\n",
        "\n",
        "kernel2 = np.array([[-0.5, -0.1, 0.5],\n",
        "                    [-0.5, 0.3, 0.5],\n",
        "                    [-0.5, -0.1, 0.5]])\n",
        "\n",
        "kernel3 = np.array([[-0.5, -0.1, 0.5],\n",
        "                    [-0.5, 0.3, 0.5],\n",
        "                    [-0.5, -0.1, 0.5]])\n",
        "\n",
        "kernel4 = np.array([[-0.5, -0.1, 0.5],\n",
        "                    [-0.5, 0.3, 0.5],\n",
        "                    [-0.5, -0.1, 0.5]])\n",
        "\n",
        "kernel5 = np.array([[-0.6, -0.4, -0.6],\n",
        "                    [-0.4, 5.2, -0.4],\n",
        "                    [-0.6, -0.4, -0.6]])\n",
        "\n",
        "# Define the CNN model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(1, (3, 3), input_shape=(image_height, image_width, 1), padding='same', kernel_initializer=tf.constant_initializer(first_kernel_weights)))\n",
        "\n",
        "'''\n",
        "# Split the input shape into three parts\n",
        "split_height = image_height // 3\n",
        "\n",
        "\n",
        "# Convolution with kernel1 for region 1\n",
        "region1 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel1))(model.output[:, :split_height, :, :])\n",
        "\n",
        "# Convolution with kernel2 for region 2\n",
        "region2 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel2))(model.output[:, split_height:2*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel3 for region 3\n",
        "region3 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel3))(model.output[:, 2*split_height:, :, :])\n",
        "\n",
        "# Merge the regions back into a single tensor\n",
        "merged_regions = tf.concat([region1, region2, region3], axis=1)\n",
        "'''\n",
        "\n",
        "'''\n",
        "#4 parts and filters\n",
        "split_height = image_height // 4\n",
        "\n",
        "# Convolution with kernel1 for region 1\n",
        "region1 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel1))(model.output[:, :split_height, :, :])\n",
        "\n",
        "# Convolution with kernel2 for region 2\n",
        "region2 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel2))(model.output[:, split_height:2*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel3 for region 3\n",
        "region3 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel3))(model.output[:, 2*split_height:3*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel4 for region 4\n",
        "region4 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel4))(model.output[:, 3*split_height:, :, :])\n",
        "\n",
        "# Merge the regions back into a single tensor\n",
        "merged_regions = tf.concat([region1, region2, region3, region4], axis=1)\n",
        "'''\n",
        "\n",
        "'''\n",
        "#6 parts and kernels\n",
        "split_height = image_height // 6\n",
        "\n",
        "# Convolution with kernel1 for region 1\n",
        "region1 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel1))(model.output[:, :split_height, :, :])\n",
        "\n",
        "# Convolution with kernel2 for region 2\n",
        "region2 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel2))(model.output[:, split_height:2*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel3 for region 3\n",
        "region3 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel3))(model.output[:, 2*split_height:3*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel4 for region 4\n",
        "region4 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel4))(model.output[:, 3*split_height:4*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel5 for region 5\n",
        "region5 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel5))(model.output[:, 4*split_height:5*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel6 for region 6\n",
        "region6 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel6))(model.output[:, 5*split_height:, :, :])\n",
        "\n",
        "# Merge the regions back into a single tensor\n",
        "merged_regions = tf.concat([region1, region2, region3, region4, region5, region6], axis=1)\n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "#7 parts and kernels\n",
        "split_height = image_height // 7\n",
        "\n",
        "# Convolution with kernel1 for region 1\n",
        "region1 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel1))(model.output[:, :split_height, :, :])\n",
        "\n",
        "# Convolution with kernel2 for region 2\n",
        "region2 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel2))(model.output[:, split_height:2*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel3 for region 3\n",
        "region3 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel3))(model.output[:, 2*split_height:3*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel4 for region 4\n",
        "region4 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel4))(model.output[:, 3*split_height:4*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel5 for region 5\n",
        "region5 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel5))(model.output[:, 4*split_height:5*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel6 for region 6\n",
        "region6 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel6))(model.output[:, 5*split_height:6*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel7 for region 7\n",
        "region7 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel7))(model.output[:, 6*split_height:, :, :])\n",
        "\n",
        "# Merge the regions back into a single tensor\n",
        "merged_regions = tf.concat([region1, region2, region3, region4, region5, region6, region7], axis=1)\n",
        "\n",
        "\n",
        "'''\n",
        "split_height = image_height // 5\n",
        "\n",
        "# Convolution with kernel1 for region 1\n",
        "region1 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel1))(model.output[:, :split_height, :, :])\n",
        "\n",
        "# Convolution with kernel2 for region 2\n",
        "region2 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel2))(model.output[:, split_height:2*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel3 for region 3\n",
        "region3 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel3))(model.output[:, 2*split_height:3*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel4 for region 4\n",
        "region4 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel4))(model.output[:, 3*split_height:4*split_height, :, :])\n",
        "\n",
        "# Convolution with kernel5 for region 5\n",
        "region5 = Conv2D(1, (3, 3), padding='same', kernel_initializer=tf.constant_initializer(kernel5))(model.output[:, 4*split_height:, :, :])\n",
        "\n",
        "# Merge the regions back into a single tensor\n",
        "merged_regions = tf.concat([region1, region2, region3, region4, region5], axis=1)\n",
        "\n",
        "model.add(LeakyReLU(alpha=0.05))\n",
        "#model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(16,3, 3))\n",
        "model.add(LeakyReLU(alpha=0.05))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Conv2D(32, 3, 3))\n",
        "model.add(LeakyReLU(alpha=0.05))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Conv2D(64, 3, 3))\n",
        "model.add(LeakyReLU(alpha=0.05))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "\n",
        "model.add(Conv2D(124, 3, 3,padding='same'))\n",
        "model.add(LeakyReLU(alpha=0.05))\n",
        "model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
        "model.add(MaxPooling2D((2,2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024))\n",
        "model.add(LeakyReLU(alpha=0.05))\n",
        "batch_size = 4\n",
        "\n",
        "model.add(Dense(124, activation='softmax'))\n",
        "\n",
        "\n",
        "print(model.summary())\n",
        "model.compile(optimizer=optimizers.Adam(learning_rate=0.001),loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "import time\n",
        "start_time=time.time()\n",
        "history=model.fit(X_train, Y_train,batch_size=4, epochs=epochs,validation_data=(X_test, Y_test))\n",
        "#history=model.fit(X_train, Y_train,batch_size=4, epochs=epochs)\n",
        "print('Training took {} seconds'.format(time.time()-start_time))\n",
        "#model.save(MODEL_NAME)\n",
        "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
        "plt.show()\n",
        "plt.savefig('/content/drive/MyDrive/DatasetGEIs/DatasetGEIs/Normal+Coats/Curves.png')\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "test_predictions = model.predict(X_test)\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(np.argmax(Y_test,axis=1), np.argmax(test_predictions,axis=1))*100\n",
        "print(\"Accuracy: \" + str(accuracy))\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "precision_score= precision_score(np.argmax(Y_test,axis=1), np.argmax(test_predictions,axis=1), average=None)\n",
        "\n",
        "recall_score= recall_score(np.argmax(Y_test,axis=1),np.argmax(test_predictions,axis=1),average=None)\n",
        "from sklearn.metrics import f1_score\n",
        "F1=f1_score(np.argmax(Y_test,axis=1),np.argmax(test_predictions,axis=1), average=None)\n",
        "from sklearn.metrics import confusion_matrix\n",
        "results = confusion_matrix(np.argmax(Y_test,axis=1),np.argmax(test_predictions,axis=1))\n",
        "print(results)\n",
        "from sklearn.metrics import classification_report\n",
        "list3=[]\n",
        "for i in range(1,125):\n",
        "    list3.append(\"Person\"+str(i))\n",
        "#print(list3)\n",
        "print(classification_report(np.argmax(Y_test,axis=1), np.argmax(test_predictions,axis=1)))\n",
        "\n",
        "list3=[]\n",
        "for i in range(1,125):\n",
        "    list3.append(\"Person\"+str(i))\n",
        "#print(list3)\n",
        "\n",
        "\n",
        "labels=pd.DataFrame()\n",
        "labels[\"ClassLabel\"]=list3\n",
        "labels[\"Precision\"]=precision_score\n",
        "labels[\"Recallvalue\"]=recall_score\n",
        "labels[\"F1Score\"]=F1\n",
        "labels.to_csv('/content/drive/MyDrive/DatasetGEIs/DatasetGEIs/Normal+Coats/File.csv')\n",
        "\n",
        "'''\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.title('Model Accuracy Results')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Train'], loc='upper left')\n",
        "plt.savefig('/content/drive/MyDrive/DatasetGEIs/DatasetGEIs/Normal+Coats/AccuracyCurve.png')\n",
        "plt.show()\n",
        "plt.close()\n",
        "'''\n",
        "'''\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.title('Model Loss Results')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['Train'], loc='upper right')\n",
        "plt.savefig('/content/drive/MyDrive/DatasetGEIs/DatasetGEIs/Normal+Coats/LossCurve.png')\n",
        "plt.show()\n",
        "plt.close()\n",
        "'''\n",
        "fig, ax = plot_confusion_matrix(conf_mat=results,colorbar=False,show_absolute=True,show_normed=False,figsize=(40,40))\n",
        "plt.savefig('/content/drive/MyDrive/DatasetGEIs/DatasetGEIs/Normal+Coats/ConfusionMatrix.png')\n",
        "plt.show()\n",
        "plt.close()\n",
        "arr = np.array(results)\n",
        "\n",
        "df = pd.DataFrame(arr)\n",
        "\n",
        "df.columns = list3\n",
        "df[\"Rows/Col\"]=list3\n",
        "df.to_csv('/content/drive/MyDrive/DatasetGEIs/DatasetGEIs/Normal+Coats/File2.csv')\n"
      ],
      "metadata": {
        "id": "CE6Pi3DJgOfP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}